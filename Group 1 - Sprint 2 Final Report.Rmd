---
title: "Group 1: Sprint 2 - Retail Analytics"
author: "Craig, Gabriel, Marina, Paul, Shashank"
date: "10/02/2020"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

We will be using Cross Industry Standard Process for Data Mining (CRISP_DM) methodology to analyze the data:

1. Business Understanding
2. Data Understanding
3. Data Exploration
4. Data Preparation
5. Modeling, Evaluation, and Deployment


### 1. BUSINESS AND ANALYTICAL PROBLEM STATEMENT

> Abstract

The goal of our analysis and report is to build personalized grocery baskets and an intelligent product
recommendation system to improve the customer online experience which will help the company keep
their existing customers but also conquer those who are still hesitating to join. In order to achieve that,
our team would provide TNG Strategic Solutions and recommendations to help them optimize the sales
of their grocery baskets, so client do not spend too much time online looking for specific items.

> Introduction

True North Groceries (TNG) is an online grocery retailer that sells a wide variety of fresh, frozen, and
Non-perishable groceries and food products. TNG has partnered with retailers (who carry these
items) and delivers them to customers. The company has a revenue sharing scheme with its
retailers, and generates sales for their retail partners. TNG also allows for exposure of certain goods
with partner brands at point of purchase. This is how they provide value. TNG believes that
leveraging data and analytics will be foundational to their long-term success.

> Business and Analytical Problem Statement

As a start-ups with a small operation team, how can a Data analytic team helps TNG leveraging data so
they provide a recommender system that will focus on providing an easy customer online experience to
their customers. The assumption that being efficient in the grocery industry requires customer focus,
TNG is rethinking how they can improve customer experience across the sales channels. The question
that TNG has brought up on the table is that, how can Data help them enhance customer experience by
providing recommendations to customers to plan their weekly groceries. They’ve asked us to present
customers with grocery baskets based on their individual purchase history and what other customers
are buying.

TNG is obsessed about providing their customers best experience possible and has asked us to improve
the shopping experience of the customer by providing them with pre- grocery baskets for weekly
groceries. Our target audience is the end customer of the website. With the dataset supplied, we can
recommend them product(s) that the customer will most likely buy and add it to the basket. Basket will
be customizable, allowing the customer to add/ edit/ delete the items. Initially, the baskets will be
created with available purchase history and with time TNG will be able to accurately predict the weekly
groceries by further capturing the purchase data of the customers.

Some analytical questions we will be answering are:

- Which products are usually reordered?
- How many items does the customer order mostly?
- Do people usually reorder the same previous ordered products?
- Time at which people usually order products?
- How many customers are there in total?
- How many orders does each customer make?
- What are the most important departments?
- Which are the most important aisles (bestselling?
- Which are the most important aisles in each department?
- What is customer’s most favorite department and aisle?
- Which customers are reordering the same item?
- What is customer buying behaviors?
- Is there and association between time of ordering and probability of reordering? If yes, what?
- Is there and association between number of orders and probability of reordering? If yes, what?


These questions will lead us to recommend the product(s) to the customer and add/edit the basket
based on purchase history:

- What products go in the basket?
- What other products is the customer likely to buy?
- What data is being used for analysis?
- Should we make use of databases, any external integrations etc.
- What is happening with the customer’s data?
- We need to put an agreement in place with the customer to understand what happens to their
data, highlighting the security risks in moving data elsewhere.


### 2. DATA UNDERSTANDING

>2.1 Data Loading and Libraries

```{r echo=T, message=FALSE}
setwd("C:/Users/gabri/OneDrive/Documents/York University - Data Analytics/CSDA 1050 - CAPSTONE")
library(data.table)
library(ggplot2)
library(rlang)
library(plyr)
library(dplyr)
library(knitr)
library(stringr)
library(DT)
library(treemap)
library(plyr)

```

>2.2 Data Importing

The source for this dataset is provided by TNG, which uses Instacart data found here: <https://www.instacart.com/datasets/grocery-shopping-2017>  


```{r}
# We use the fread() function because it processes files much faster than the traditional read() function
departments <- fread("departments.csv")  
aisles <- fread("aisles.csv")
products <- fread("products.csv")  
orders <- fread("orders.csv")  
order_products_prior <- fread("order_products__prior.csv")  
order_products_train <- fread("order_products__train.csv")
```

>2.3 Data Dictionary

- **products** (50k rows)
- *product_id*: product identifier
- *product_name*: name of the product
- *aisle_id*: foreign key
- *department_id*: foreign key

- **aisles** (134 rows)
- *aisle_id*: aisle identifier
- *aisle*: the name of the aisle

- **departments** (21 rows)
- *department_id*: department identifier
- *department*: the name of the department


- **orders** (3.4m rows, 206k users)
- *order_id*: order identifier
- *user_id*: customer identifier
- *eval_set*: which evaluation set this order belongs in (see SET described below)
- *order_number*: the order sequence number for this user (1 = first, n = nth)
- *order_dow*: the day of the week the order was placed on
- *order_hour_of_day*: the hour of the day the order was placed on
- *days_since_prior*: days since the last order, capped at 30 (with NAs for *order_number* = 1)

- where SET is one of the four following evaluation sets (**eval_set** in orders):

- **prior**: orders prior to that users most recent order (~3.2m orders)
- **train**: training data supplied to participants (~131k orders)
- **test**: test data reserved for machine learning competitions (~75k orders)

>2.4 Dataset Relations

![Relational Database from Instacart](Instacart Data.png) 

From this database model we realize that there will be various manipulation techniques that we should carry out in the following sections (such as merging and joining by a certain key) in order to get the most from our datasets.


### 3. DATA EXPLORATION

Here we will be doing exploratory work on our data to get a sense of what data entails, what variables are useful,
and what primitive patterns are occurring in our customers' data.

>3.1 Preliminary Data Overview


```{r}

# All aisle_id's with their corresponding description
glimpse(aisles)

# All department_id's with their corresponding department name
glimpse(departments)

# All orders
glimpse(orders)

# All products
glimpse(products)

# order_id and product_id for both the training data and prior data
glimpse(order_products_train)
glimpse(order_products_prior)


```

>3.2 Data Exploration of Orders

Now we want to know, when do people order groceries? We'll subset our orders data through different variables to get a sense
of the data. We will first take a look at *when* customers buy groceries online.

```{r echo=FALSE, warning=FALSE}
orders %>% 
  ggplot(aes(x=order_hour_of_day)) + 
  geom_histogram(stat="count",fill="purple")
```

There is a clear effect of hour of day on order volume. 
We verify that the peak order is during the 8AM and 8 PM window. 
This can help determine the scalability factor for the website and also helps determine the maintainance windows.

Next we check **orders** by week.

```{r echo=FALSE, warning=FALSE}
orders %>% 
  ggplot(aes(x=order_dow)) + 
  geom_histogram(stat="count",fill="red")
```

We can see that the order spread are more towards day 0 and 1 vs the rest of the week when the orders are very consistent.
Assumption we make here is that people buy most during weekends and hence Day 0 is Saturday and Day 1 is Sunday and so forth.


Next we check the **number of days** since the last order.

```{r echo=FALSE, warning=FALSE}
orders %>% 
  ggplot(aes(x=days_since_prior_order)) + 
  geom_histogram(stat="count",fill="blue")
```

We notice the peak on 8 , Which helps understand the buying behaviour of the customers. Customers have to be kept engaged. 
At 14 days  and beyond the number of repeat orders is significantly low than in 1st 14 days. 
The peak at 30 is not considered as it is the cumulative from the 30+ days.


Now that we've understood customer shopping habits regarding time, we dig deeper into the prior and training datasets.

We first check how many items in each order there are in the **training** set.

```{r echo=FALSE, warning=FALSE}
order_products_train %>% 
  group_by(order_id) %>% 
  summarize(n_items = last(add_to_cart_order)) %>%
  ggplot(aes(x=n_items))+
  geom_histogram(stat="count",fill="purple") + 
  geom_rug()+
  coord_cartesian(xlim=c(0,80))
```

There are 75000 orders with 3-8 items
50000 orders are with 12 items or more.
Peak is at 5 items.
We can see that people often order around 6 items per cart.

Now we check the items in the the **prior** set.

```{r echo=FALSE, warning=FALSE}
order_products_prior %>% 
  group_by(order_id) %>% 
  summarize(n_items = last(add_to_cart_order)) %>%
  ggplot(aes(x=n_items))+
  geom_histogram(stat="count",fill="red") + 
  geom_rug() + 
  coord_cartesian(xlim=c(0,80))
```

There are 200000 orders with 3-8 items.
150000 orders are with 10 items or less.
Peak is at 5 items.
We can see that people often order around 5 and 6 items.

Now we check the association between **time of last order** and probability of **reorder**.

```{r echo=FALSE, warning=FALSE}
order_products_train %>% 
  left_join(orders,by="order_id") %>% 
  group_by(days_since_prior_order) %>%
  summarize(mean_reorder = mean(reordered)) %>%
  ggplot(aes(x=days_since_prior_order,y=mean_reorder))+
  geom_bar(stat="identity",fill="purple")
```

There is a huge probability that customers will come back to order the same day. Probably because they forgot something to order. Recommending products will see a change in this number.

30 days and after (data is capped at 30), the probablity of reorder is significantly less. Therefore, email should be timed weekly or fortnightly. Monthly email may not be that successful in bringing the customers back.

>3.3 Data Exploration of Products

Now we explore our **products** dataset. We start off by checking the top 20 most popular products 

```{r, warning=FALSE}
tmp <- order_products_train %>% 
  group_by(product_id) %>% 
  summarize(count = n()) %>% 
  top_n(20, wt = count) %>%
  left_join(select(products,product_id,product_name),by="product_id") %>%
  arrange(desc(count)) 
```

```{r echo=FALSE, warning=FALSE}
tmp %>% 
  ggplot(aes(x=reorder(product_name,-count), y=count))+
  geom_bar(stat="identity",fill="purple")+
  theme(axis.text.x=element_text(angle=90, hjust=1),axis.title.x = element_blank())
```

Banana (Organic and otherwise is the top seller) Followed by strawberries, spinach, avocado and lemon. 
By count there is an order of magnitude difference between bananas, strawberries combined, and organic spinach and lemon.
Because these are bestsellers, these should be recommended to customer in a "What other customers are buying" category.

Now we check how often do people order the same items again?

```{r warning=FALSE}
tmp <- order_products_train %>% 
  group_by(reordered) %>% 
  summarize(count = n()) %>% 
  mutate(reordered = as.factor(reordered)) %>%
  mutate(proportion = count/sum(count))
kable(tmp)

```

There is a 60/40 split between items that are re-ordered (=1) and that are ordered only once (=0).

Now we check which products are most likely to be ordered again?

```{r warning=FALSE}
tmp <-order_products_train %>% 
  group_by(product_id) %>% 
  summarize(proportion_reordered = mean(reordered), n=n()) %>% 
  filter(n>40) %>% 
  top_n(10,wt=proportion_reordered) %>% 
  arrange(desc(proportion_reordered)) %>% 
  left_join(products,by="product_id")
```

```{r echo=FALSE, warning=FALSE}
tmp %>% 
  ggplot(aes(x=reorder(product_name,-proportion_reordered), y=proportion_reordered))+
  geom_bar(stat="identity",fill="purple")+
  theme(axis.text.x=element_text(angle=90, hjust=1),axis.title.x = element_blank())+coord_cartesian(ylim=c(0.85,0.95))
```

Milk is the most likely product to be ordered again.
If the customer has a history of buying milk then milk should be recommended to the customer.
Bananas, Tortillas and Orange juice are closely following the trend and should be the candidates for the same.

Next we are interested in the percentage of orders that are organic vs. not organic.

```{r warning=FALSE}
products <- products %>% 
  mutate(organic=ifelse(str_detect(str_to_lower(products$product_name),'organic'),"organic","not organic"), organic= as.factor(organic))

tmp <- order_products_train %>% 
  left_join(products, by="product_id") %>% 
  group_by(organic) %>% 
  summarize(count = n()) %>% 
  mutate(proportion = count/sum(count))
```

```{r echo=FALSE, warning=FALSE}
tmp %>% 
  ggplot(aes(x=organic,y=count, fill=organic))+
  geom_bar(stat="identity")
```

Not surprisingly, non-organic sales (by count) is more than organic sales.

Lastly, we check the probability of reordering, whether if the item is organic or non-organic.

```{r warning=FALSE}
tmp <- order_products_train %>% left_join(products,by="product_id") %>% group_by(organic) %>% summarize(mean_reordered = mean(reordered))
```

```{r echo=FALSE, warning=FALSE}
tmp %>% 
  ggplot(aes(x=organic,fill=organic,y=mean_reordered))+geom_bar(stat="identity")

```


Customers who buy organic are more likely to buy again. This insight will help profile the customers as well. 
Adding customer profiles to our data may help undersatnd who are the customers that buy organic. They are more likely to buy again.  


>3.4 Tree map vizualization for further insights

For the final section of this Data Exploration, we will do some explorations using **tree map** distributions. This will allow us to get the big picture of our products, based on its respective department and aisle, and see how big of a proportion a category could yield. 

```{r warning=FALSE}
tmp <- products %>% group_by(department_id, aisle_id) %>% summarize(n=n())
tmp <- tmp %>% left_join(departments,by="department_id")
tmp <- tmp %>% left_join(aisles,by="aisle_id")

tmp2<-order_products_train %>% 
  group_by(product_id) %>% 
  summarize(count=n()) %>% 
  left_join(products,by="product_id") %>% 
  ungroup() %>% 
  group_by(department_id,aisle_id) %>% 
  summarize(sumcount = sum(count)) %>% 
  left_join(tmp, by = c("department_id", "aisle_id")) %>% 
  mutate(onesize = 1)

#we will be using the treemap() function to produce our visualizations
```

```{r echo=FALSE, warning=FALSE}
treemap(tmp,index=c("department","aisle"),vSize="n",title="",palette="Set3",border.col="#FFFFFF")
```

Here we see the distribution of products within these aisles, and each aisle within each department. Each different department is colour coded, with each aisle corresponding to a department, depicted as the boxes within these colour groups.

Next, we will again use the treemap() function to investigate the distribution of products within their aisles/departments, but this time it will be based on their **frequency**

```{r echo=FALSE, warning=FALSE}
treemap(tmp2,index=c("department","aisle"),vSize="sumcount",title="",palette="Set3",border.col="#FFFFFF")
```


### 4. DATA PREPARATION

Here we will take a more technical approach to checking, investigating, and preparing our data.

>4.1 Data Checking

We first begin to clean and investigate our dataset by quickly browsing the first two smallest datasets: **departments** and **aisles**.

```{r warning=FALSE}
table(departments$department)
table(aisles$aisle)
```

We see that departments has a complete dataset, with the exception of the department_id row 21 labeled **"missing"**
This occurs similarly in the aisles dataset, row #100.

To see how big of an impact this could be on our results, we quickly check the frequency distribution of our products by aisle and dept.

```{r warning=FALSE}
table(products$department_id,useNA = "ifany")
prop.table(table(products$department_id))
```

```{r warning=FALSE,results="hide"}
#we run the same code for the aisles data
table(products$aisle_id,useNA = "ifany")
prop.table(table(products$aisle_id)) 
```

From these results we can see that the proportion of the "missing" department_id labels in products is only 2.5%. However, the proportion of "missing" in aisle_id is also 2.5%, but much larger in comparison due to there being 134 aisles.
We also notice that there are no NA's for either of these which is good.

We will decide to include them for now, but it is good to keep these results in mind when doing data analysis per department or aisle.

Now, for investigative purposes, we also check what sorts of items are in these "missing" columns.

```{r warning=FALSE}
missing_products <- subset(products, aisle_id==100)
kable(head(missing_products))
```

We notice that many of theses items are common, others no so much, but they could potentially yield useful and valuable results. Regardless of having a missing label, Organic Honeycrisp Apples (as an example) have missing department_id/aisle_id, but are very common and frequently bought grocery items.

>4.2 Data Cleaning

We also check each row of our products dataset and note that it corresponds to each product_id, meaning there are no missing product_id.

```{r warning=FALSE}
last(products$product_id)
dim(products)
```

Next we check to see if all product names are intact, by first setting any blanks in product_name to NA's, then checking if there are any NA's.

```{r warning=FALSE}
products$product_name[products$product_name==""] <- NA
table(is.na(products$product_name))
```

For the last dataset **orders**, we do another quick check to see if there are any missing or NA values.

```{r warning=FALSE}
table(is.na(orders))
```

Though the TRUE is alarming at first, we investigate the data and we realize that these NA's occur only when the first_days_since_prior repeats for each new customer, which makes sense, logically speaking.

>4.3 Data Merging and Imputation

To make our lives easier, here we will merge some datasets together to be used in the next section for our model.

The first merge will be products with aisles and departments, namely so we can see to which aisle and department each product pertains to.

```{r warning=FALSE}
products_merged <- merge(products, departments, by.x="department_id",by.y="department_id")
products_merged <- merge(products_merged, aisles, by.x="aisle_id",by.y="aisle_id")

#Then we remove the first two columns (aisle.id and department.id) as the id #s are not of use to us
products_merged <- subset(products_merged, select = -c(aisle_id,department_id))
kable(head(products_merged))
```


The next data merge will be with this new products_merged dataset and the **prior** and **train** orders.

```{r warning=FALSE}
missing_product_check <- anti_join(order_products_prior,products_merged,by = "product_id")
missing_product_check2 <- anti_join(order_products_train,products_merged,by = "product_id")
head(missing_product_check)
```

We see that since there are 0 rows, that neither the order_products_prior nor the order_products_train contain any unknown product_id's (from the products dataset), so we can safely continue our data cleaning assured that there will be no unmatched product_id number.

Now for the dataset that we will *actually* use in our analysis; we merge together **orders** with the **prior** and **training** sets based on order_id.

We do this in order to see each user's specific orders AND each order's specific products within.

```{r warning=FALSE}
prior_and_train <- rbind(order_products_prior,order_products_train)
new_orders <- left_join(orders,prior_and_train,by = "order_id")
kable(head(new_orders))
```


Lastly, we merge the orders with the previously merged products to get the description, aisle, and department of each product for each order/user.

```{r warning=FALSE}
order_descriptions <- left_join(new_orders,products_merged, by = "product_id")
kable(head(order_descriptions))
```


>4.4 Data Encoding

For the final step will also convert/recode some column variables from integer to factors for pre-processing, as it will makes things more streamlined (less prone to errors) when it will be time for plotting and using our machine learning techniques and algorithms.

```{r warning=FALSE}
mining_data <- order_descriptions

mining_data$order_id <- as.numeric(mining_data$order_id)
mining_data$user_id <- as.numeric(mining_data$user_id)
mining_data$product_name <- as.factor(mining_data$product_name)
```

Now we can begin the next step which is fitting the initial model!